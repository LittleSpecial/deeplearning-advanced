# robot_control_panel.py
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import threading
import requests
import socketio
import cv2
import time  # æ·»åŠ timeæ¨¡å—
import os    # æ·»åŠ osæ¨¡å—

from ultralytics import YOLO

import face_recognition
import pickle
import numpy as np

# ---------------------------
# é…ç½®
# ---------------------------
PI_ADDRESS = 'http://192.168.43.14:5000'  # æ”¹æˆä½ çš„æ ‘è“æ´¾IP
VIDEO_STREAM_URL = PI_ADDRESS + '/video_feed'
COMMAND_URL = PI_ADDRESS + '/command'  # æ·»åŠ å‘½ä»¤å‘é€URL


# YOLOæ¨¡å‹é…ç½® - ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹
print("[INFO] Loading YOLO model...")
try:
    # é¦–å…ˆå°è¯•åŠ è½½è‡ªå®šä¹‰é¥®æ–™æ£€æµ‹æ¨¡å‹
    custom_model_path = "yolo_weights/best.pt"
    if os.path.exists(custom_model_path):
        yolo_model = YOLO(custom_model_path)
        print(f"[INFO] Custom beverage detection model loaded from {custom_model_path}")
        model_type = "custom"
    else:
        # å¦‚æœè‡ªå®šä¹‰æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹
        yolo_model = YOLO('yolov8n.pt')
        print("[INFO] Official YOLOv8n model loaded (general object detection)")
        model_type = "official"
        
    print(f"[INFO] Model can detect: {list(yolo_model.names.values())}")
except Exception as e:
    print(f"[ERROR] Failed to load YOLO model: {e}")
    yolo_model = None
    model_type = "none"

# è‡ªåŠ¨æ¨¡å¼ç›¸å…³é…ç½®
FACE_CONFIDENCE_THRESHOLD = 0.6
AUTO_MODE_ENABLED = False
current_state = "IDLE"
recognized_person = None
last_recognition_time = 0
RECOGNITION_COOLDOWN = 3  # ç§’ï¼Œé¿å…é‡å¤è¯†åˆ«

# æ‘„åƒå¤´è§’åº¦æ§åˆ¶é…ç½®
camera_position = "middle"  # å½“å‰æ‘„åƒå¤´ä½ç½®: "up", "down", "middle"
last_camera_command_time = 0
CAMERA_MOVE_COOLDOWN = 2  # æ‘„åƒå¤´ç§»åŠ¨æŒ‡ä»¤é—´éš”ï¼Œé¿å…é¢‘ç¹å‘é€
auto_stage = "SEARCHING_PERSON"  # è‡ªåŠ¨æ¨¡å¼é˜¶æ®µ: "SEARCHING_PERSON", "SEARCHING_DRINK", "ROTATING_SEARCH", "MOVING_TO_DRINK", "GRABBING", "RETURNING", "SERVING"

# 360åº¦æœç´¢é…ç½®
rotation_angle = 0  # å½“å‰æ—‹è½¬è§’åº¦
ROTATION_STEP = 45  # æ¯æ¬¡æ—‹è½¬45åº¦
ROTATION_DELAY = 2  # æ¯ä¸ªè§’åº¦åœç•™2ç§’æ£€æµ‹ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
last_rotation_time = 0
drink_found_position = None  # å‘ç°é¥®æ–™æ—¶çš„è§’åº¦
original_position = {"x": 0, "y": 0, "angle": 0}  # æœºå™¨äººåŸå§‹ä½ç½®

# äººè„¸è·Ÿè¸ªç¨³å®šæ€§é…ç½®
face_tracking_data = {}  # å­˜å‚¨æ¯ä¸ªäººè„¸çš„è·Ÿè¸ªä¿¡æ¯
FACE_TRACKING_FRAMES = 2  # å‡å°‘åˆ°2å¸§å°±æ˜¾ç¤ºç¨³å®šçš„æ¡†
FACE_DISAPPEAR_FRAMES = 8  # 8å¸§åç§»é™¤è·Ÿè¸ªï¼ˆçº¦0.53ç§’ï¼‰

DATABASE_PATH = "face_database.pkl"  # ä¿®æ­£è·¯å¾„  
try:
    with open(DATABASE_PATH, "rb") as f:
        database = pickle.load(f)
    known_names = list(database.keys())
    known_encodings = [data["embedding"] for data in database.values()]
    print("[INFO] Face database loaded.")
    print(f"[INFO] Found {len(known_names)} registered faces: {known_names}")
except FileNotFoundError:
    print("[ERROR] Face database not found. Please run 01_enroll_faces.py first.")
    known_names = []
    known_encodings = []
    database = {}

# ---------------------------
# å»ºç«‹ SocketIO å®¢æˆ·ç«¯è¿æ¥
# ---------------------------
sio = socketio.Client()

try:
    sio.connect(PI_ADDRESS)
    print(f"[SOCKETIO] Connected to {PI_ADDRESS}")
except Exception as e:
    print(f"[SOCKETIO ERROR] Failed to connect: {e}")

@sio.event
def connect():
    print("[SOCKETIO] Connected to server")

@sio.event
def disconnect():
    print("[SOCKETIO] Disconnected from server")

@sio.event
def connect_error(data):
    print(f"[SOCKETIO ERROR] Connection failed: {data}")

# ---------------------------
# æ¨¡å¼åˆ‡æ¢ï¼ˆå‘é€ POST è¯·æ±‚ï¼‰
# ---------------------------
def switch_mode(mode):
    global AUTO_MODE_ENABLED
    try:
        # å¦‚æœæœ‰app.pyçš„æ¨¡å¼åˆ‡æ¢æ¥å£ï¼Œä½¿ç”¨å®ƒ
        requests.post(PI_ADDRESS + '/switch_mode', json={'mode': mode}, timeout=2)
        print(f"[MODE] Switched to {mode}")
        AUTO_MODE_ENABLED = (mode == 'auto')
    except Exception as e:
        print(f"[ERROR] Mode switch failed: {e}")
        # å¦‚æœapp.pyæ²¡æœ‰æ¨¡å¼åˆ‡æ¢æ¥å£ï¼Œåªåœ¨æœ¬åœ°åˆ‡æ¢
        AUTO_MODE_ENABLED = (mode == 'auto')
        print(f"[MODE] Local mode switched to {mode}")

def send_robot_command(command):
    """å‘é€å‘½ä»¤åˆ°æœºå™¨äºº"""
    try:
        # é¦–å…ˆå°è¯•å‘é€åˆ°pi_robot_server.pyçš„æ¥å£
        response = requests.post(COMMAND_URL, json={'command': command}, timeout=1)
        if response.status_code == 200:
            print(f"[CMD] Sent to robot: {command}")
            return True
    except Exception as e:
        print(f"[CMD ERROR] Failed to send command {command}: {e}")
    
    # å¦‚æœrobot serverä¸å¯ç”¨ï¼Œå°è¯•é€šè¿‡SocketIOå‘é€
    try:
        if command == "MOVE:FORWARD":
            sio.emit('keydown', 'w')
        elif command == "MOVE:BACKWARD":
            sio.emit('keydown', 's')
        elif command == "TURN:LEFT":
            sio.emit('keydown', 'a')
        elif command == "TURN:RIGHT":
            sio.emit('keydown', 'd')
        elif command == "STATUS:Idle":
            sio.emit('keyup', 'x')
        print(f"[CMD] Sent via SocketIO: {command}")
        return True
    except Exception as e:
        print(f"[CMD ERROR] SocketIO command failed: {e}")
        return False

def control_camera_angle(position):
    """æ§åˆ¶æ‘„åƒå¤´è§’åº¦ - åªæœ‰UPå’ŒDOWNä¸¤ä¸ªä½ç½®"""
    global camera_position, last_camera_command_time
    current_time = time.time()
    
    # é¿å…é¢‘ç¹å‘é€æ‘„åƒå¤´æ§åˆ¶æŒ‡ä»¤
    if current_time - last_camera_command_time < CAMERA_MOVE_COOLDOWN:
        return
    
    if camera_position == position:
        return  # å·²ç»åœ¨ç›®æ ‡ä½ç½®
    
    print(f"[CAMERA] Moving camera from {camera_position} to {position}")
    
    try:
        if position == "up":
            # å‘é€å‘ä¸ŠæŒ‡ä»¤ - ç”¨äºäººè„¸è¯†åˆ«
            sio.emit('keydown', 'u')
            time.sleep(0.2)  # ç¨é•¿ä¸€ç‚¹ç¡®ä¿åŠ¨ä½œå®Œæˆ
            sio.emit('keyup', 'u')
            camera_position = "up"
            print("[CAMERA] Camera moved UP for face recognition")
            
        elif position == "down":
            # å‘é€å‘ä¸‹æŒ‡ä»¤ - ç”¨äºé¥®æ–™è¯†åˆ«
            sio.emit('keydown', 'j')
            time.sleep(0.2)  # ç¨é•¿ä¸€ç‚¹ç¡®ä¿åŠ¨ä½œå®Œæˆ
            sio.emit('keyup', 'j')
            camera_position = "down"
            print("[CAMERA] Camera moved DOWN for beverage detection")
            
        elif position == "middle":
            # å‘é€ä¸­é—´ä½ç½®æŒ‡ä»¤
            sio.emit('keydown', 'n')
            time.sleep(0.2)
            sio.emit('keyup', 'n')
            camera_position = "middle"
            print("[CAMERA] Camera moved to MIDDLE position")
            
        last_camera_command_time = current_time
        
    except Exception as e:
        print(f"[CAMERA ERROR] Failed to move camera to {position}: {e}")

def rotate_robot(direction, duration=1.0):
    """æ—‹è½¬æœºå™¨äºº"""
    try:
        print(f"[ROBOT DEBUG] Starting rotation {direction} for {duration}s")
        if direction == "left":
            sio.emit('keydown', 'a')
            time.sleep(duration)
            sio.emit('keyup', 'a')
            print(f"[ROBOT] Rotated LEFT for {duration}s")
        elif direction == "right":
            sio.emit('keydown', 'd')
            time.sleep(duration)
            sio.emit('keyup', 'd')
            print(f"[ROBOT] Rotated RIGHT for {duration}s")
        time.sleep(0.3)  # å‡å°‘åœé¡¿æ—¶é—´ï¼Œè®©æ—‹è½¬æ›´è¿ç»­
        print(f"[ROBOT DEBUG] Rotation {direction} completed")
    except Exception as e:
        print(f"[ROBOT ERROR] Failed to rotate {direction}: {e}")

def move_robot(direction, duration=1.0):
    """ç§»åŠ¨æœºå™¨äºº"""
    try:
        if direction == "forward":
            sio.emit('keydown', 'w')
            time.sleep(duration)
            sio.emit('keyup', 'w')
            print(f"[ROBOT] Moved FORWARD for {duration}s")
        elif direction == "backward":
            sio.emit('keydown', 's')
            time.sleep(duration)
            sio.emit('keyup', 's')
            print(f"[ROBOT] Moved BACKWARD for {duration}s")
        time.sleep(0.5)  # åœé¡¿è®©æœºå™¨äººç¨³å®š
    except Exception as e:
        print(f"[ROBOT ERROR] Failed to move {direction}: {e}")

def grab_drink():
    """æŠ“å–é¥®æ–™"""
    try:
        sio.emit('keydown', 'g')
        time.sleep(0.2)
        sio.emit('keyup', 'g')
        print("[ROBOT] Grabbing drink...")
        time.sleep(3)  # ç­‰å¾…æŠ“å–å®Œæˆ
    except Exception as e:
        print(f"[ROBOT ERROR] Failed to grab drink: {e}")

def serve_drink():
    """é€’é€é¥®æ–™"""
    try:
        sio.emit('keydown', 'r')
        time.sleep(0.2)
        sio.emit('keyup', 'r')
        print("[ROBOT] Serving drink...")
        time.sleep(3)  # ç­‰å¾…é€’é€å®Œæˆ
    except Exception as e:
        print(f"[ROBOT ERROR] Failed to serve drink: {e}")

def return_to_origin():
    """è¿”å›åŸç‚¹é¢å‘é¡¾å®¢"""
    global drink_found_position
    try:
        # åé€€ä¸€æ®µè·ç¦»
        move_robot("backward", 2.0)
        
        # å¦‚æœè®°å½•äº†å‘ç°é¥®æ–™çš„è§’åº¦ï¼Œéœ€è¦åå‘æ—‹è½¬å›åˆ°åŸç‚¹
        if drink_found_position is not None:
            # è®¡ç®—éœ€è¦åå‘æ—‹è½¬çš„è§’åº¦
            return_angle = (360 - drink_found_position) % 360
            if return_angle > 180:
                # å·¦è½¬æ›´çŸ­
                rotation_time = (360 - return_angle) * 0.8 / 45  # æ¯45åº¦0.8ç§’
                rotate_robot("left", rotation_time)
            else:
                # å³è½¬æ›´çŸ­  
                rotation_time = return_angle * 0.8 / 45
                rotate_robot("right", rotation_time)
            
            print(f"[ROBOT] Returned to origin from angle {drink_found_position}Â°")
        else:
            print("[ROBOT] Returned to origin")
            
    except Exception as e:
        print(f"[ROBOT ERROR] Failed to return to origin: {e}")

def check_for_target_drink(detected_objects, user_preference):
    """æ£€æŸ¥æ˜¯å¦å‘ç°ç›®æ ‡é¥®æ–™"""
    if not detected_objects:
        return False
    
    # é¥®æ–™ç±»åˆ«æ˜ å°„
    drink_keywords = {
        'bottle': ['bottle', 'water', 'soda', 'drink'],
        'cup': ['cup', 'coffee', 'tea', 'mug'],
        'wine glass': ['wine', 'glass', 'alcohol'],
        'bowl': ['bowl', 'soup']
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¥®æ–™ç›¸å…³ç‰©ä½“
    found_drinks = []
    for obj in detected_objects:
        if obj.lower() in ['bottle', 'cup', 'wine glass', 'bowl']:
            found_drinks.append(obj)
    
    if found_drinks:
        print(f"[DRINK DETECTION] Found drinks: {found_drinks}")
        
        # å¦‚æœä½¿ç”¨å®˜æ–¹æ¨¡å‹ï¼Œè¦æ›´ä¸¥æ ¼åœ°åŒ¹é…ç”¨æˆ·åå¥½
        if model_type == "official":
            # ç²¾ç¡®åŒ¹é…ï¼šå¿…é¡»æ‰¾åˆ°ä¸ç”¨æˆ·åå¥½å®Œå…¨åŒ¹é…çš„é¥®æ–™
            for drink in found_drinks:
                for keyword in drink_keywords.get(drink, []):
                    if keyword.lower() in user_preference.lower():
                        print(f"[MATCH] Found exact target drink '{drink}' matching preference '{user_preference}'")
                        return True
            
            # å®˜æ–¹æ¨¡å‹ä¸‹ï¼Œå¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œç»§ç»­æœç´¢
            print(f"[NO MATCH] Found drinks {found_drinks} but none match preference '{user_preference}', continuing search...")
            return False
        else:
            # è‡ªå®šä¹‰æ¨¡å‹ä¸‹ï¼Œæ£€æµ‹åˆ°ä»»ä½•é¥®æ–™éƒ½è®¤ä¸ºæ˜¯ç›®æ ‡
            for drink in found_drinks:
                for keyword in drink_keywords.get(drink, []):
                    if keyword.lower() in user_preference.lower():
                        print(f"[MATCH] Found target drink '{drink}' matching preference '{user_preference}'")
                        return True
            
            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½†æœ‰é¥®æ–™ï¼Œä¹Ÿè¿”å›Trueï¼ˆé€šç”¨é¥®æ–™æ£€æµ‹ï¼‰
            print(f"[MATCH] Found general drinks, proceeding...")
            return True
    
    return False

def update_face_tracking(face_detections, frame_number):
    """æ›´æ–°äººè„¸è·Ÿè¸ªæ•°æ®ï¼Œå‡å°‘é—ªçƒ"""
    global face_tracking_data
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•äººè„¸ï¼Œéœ€è¦å¤„ç†ç°æœ‰è·Ÿè¸ªæ•°æ®
    if not face_detections:
        print(f"[FACE TRACKING] No faces detected in frame {frame_number}")
        # æ¸…ç†é•¿æ—¶é—´æ¶ˆå¤±çš„äººè„¸
        to_remove = []
        for face_id, tracking_info in face_tracking_data.items():
            # å‡å°‘ç¨³å®šæ€§åˆ†æ•°
            tracking_info['stable_frames'] = max(0, tracking_info['stable_frames'] - 1)
            
            # å¦‚æœäººè„¸æ¶ˆå¤±æ—¶é—´è¿‡é•¿ï¼Œç§»é™¤
            if frame_number - tracking_info['last_seen_frame'] > FACE_DISAPPEAR_FRAMES:
                to_remove.append(face_id)
                print(f"[FACE TRACKING] Marking face {face_id} for removal - not seen for {frame_number - tracking_info['last_seen_frame']} frames")
        
        for face_id in to_remove:
            del face_tracking_data[face_id]
            print(f"[FACE TRACKING] Removed disappeared face: {face_id}")
        
        return []
    
    # æ ‡è®°å½“å‰å¸§æ£€æµ‹åˆ°çš„äººè„¸ID
    current_frame_faces = set()
    
    # è®¡ç®—å½“å‰æ£€æµ‹åˆ°çš„äººè„¸ä¸å·²è·Ÿè¸ªäººè„¸çš„è·ç¦»
    current_faces = []
    
    for detection in face_detections:
        name, box, preference = detection
        left, top, right, bottom = box
        center_x = (left + right) // 2
        center_y = (top + bottom) // 2
        
        # å¯»æ‰¾æœ€è¿‘çš„å·²è·Ÿè¸ªäººè„¸
        best_match = None
        min_distance = float('inf')
        
        for face_id, tracking_info in face_tracking_data.items():
            if tracking_info['name'] == name:
                last_center = tracking_info['center']
                distance = ((center_x - last_center[0]) ** 2 + (center_y - last_center[1]) ** 2) ** 0.5
                if distance < min_distance and distance < 100:  # æ”¾å®½åˆ°100åƒç´ å†…è®¤ä¸ºæ˜¯åŒä¸€ä¸ªäººè„¸
                    min_distance = distance
                    best_match = face_id
        
        if best_match:
            # æ›´æ–°å·²å­˜åœ¨çš„äººè„¸è·Ÿè¸ª
            face_tracking_data[best_match].update({
                'box': box,
                'center': (center_x, center_y),
                'last_seen_frame': frame_number,
                'stable_frames': min(face_tracking_data[best_match]['stable_frames'] + 2, 20),  # å¿«é€Ÿå¢åŠ ç¨³å®šæ€§
                'preference': preference
            })
            current_faces.append(best_match)
            current_frame_faces.add(best_match)
        else:
            # åˆ›å»ºæ–°çš„äººè„¸è·Ÿè¸ª
            face_id = f"{name}_{frame_number}"
            face_tracking_data[face_id] = {
                'name': name,
                'box': box,
                'center': (center_x, center_y),
                'last_seen_frame': frame_number,
                'stable_frames': 2,  # ç›´æ¥ç»™è¾ƒé«˜çš„åˆå§‹ç¨³å®šæ€§
                'preference': preference
            }
            current_faces.append(face_id)
            current_frame_faces.add(face_id)
    
    # æ¸…ç†é•¿æ—¶é—´æ¶ˆå¤±çš„äººè„¸
    to_remove = []
    for face_id, tracking_info in face_tracking_data.items():
        if frame_number - tracking_info['last_seen_frame'] > FACE_DISAPPEAR_FRAMES:
            to_remove.append(face_id)
        elif face_id not in current_frame_faces:
            # æœªæ£€æµ‹åˆ°çš„äººè„¸å‡å°‘ç¨³å®šæ€§
            tracking_info['stable_frames'] = max(0, tracking_info['stable_frames'] - 1)
    
    for face_id in to_remove:
        del face_tracking_data[face_id]
        print(f"[FACE TRACKING] Removed disappeared face: {face_id}")
    
    return current_faces

def get_stable_faces():
    """è·å–ç¨³å®šè·Ÿè¸ªçš„äººè„¸ï¼ˆå‡å°‘é—ªçƒï¼‰"""
    stable_faces = []
    
    for face_id, tracking_info in face_tracking_data.items():
        # åªè¦ç¨³å®šæ€§åˆ†æ•°è¾¾åˆ°é˜ˆå€¼å°±æ˜¾ç¤ºï¼Œä¸è€ƒè™‘æ—¶é—´å› ç´ 
        if tracking_info['stable_frames'] >= FACE_TRACKING_FRAMES:
            stable_faces.append((tracking_info['name'], tracking_info['box'], tracking_info['preference']))
    
    return stable_faces

# ---------------------------
# é”®ç›˜æ§åˆ¶ï¼ˆåªåœ¨æ‰‹åŠ¨æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰
# ---------------------------
def on_key_press(event):
    if AUTO_MODE_ENABLED:
        print("[INFO] Keyboard control disabled in AUTO mode")
        return
        
    k = event.char.lower()
    if k in ['w', 'a', 's', 'd']:
        sio.emit('keydown', k)
        # åŒæ—¶æ˜¾ç¤ºæŒ‰é”®çŠ¶æ€
        if k == 'w':
            status_label.config(text="Status: Moving Forward")
        elif k == 's':
            status_label.config(text="Status: Moving Backward")
        elif k == 'a':
            status_label.config(text="Status: Turning Left")
        elif k == 'd':
            status_label.config(text="Status: Turning Right")
    elif k in ['g', 'r']:
        sio.emit('keydown', k)
        status_label.config(text=f"Status: Gripper {'Open' if k == 'g' else 'Close'}")
    elif event.keysym == 'Up':
        sio.emit('keydown', 'u')
        status_label.config(text="Status: Moving Up")
    elif event.keysym == 'Down':
        sio.emit('keydown', 'j')
        status_label.config(text="Status: Moving Down")

def on_key_release(event):
    if AUTO_MODE_ENABLED:
        return
        
    k = event.char.lower()
    if k in ['w', 'a', 's', 'd']:
        sio.emit('keyup', k)
        status_label.config(text="Status: Manual Mode - Use WASD keys to control")

# ---------------------------
# è§†é¢‘æµæ›´æ–°çº¿ç¨‹
# ---------------------------
def update_video():
    global current_state, recognized_person, last_recognition_time, auto_stage
    global last_rotation_time, rotation_angle, drink_found_position  # æ·»åŠ å…¨å±€å˜é‡å£°æ˜
    cap = cv2.VideoCapture(VIDEO_STREAM_URL)
    
    # ä¼˜åŒ–è§†é¢‘æµè®¾ç½®
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²ï¼Œè·å–æœ€æ–°å¸§
    
    # å¸§ç‡æ§åˆ¶
    fps_limit = 15  # é™åˆ¶ä¸º15FPSï¼Œé¿å…é—ªçƒ
    frame_time = 1.0 / fps_limit
    last_frame_time = time.time()
    
    # AIå¤„ç†é¢‘ç‡æ§åˆ¶
    ai_process_interval = 3  # æ¯3å¸§å¤„ç†ä¸€æ¬¡AI
    frame_count = 0
    
    while not stop_event.is_set():
        current_time = time.time()
        
        # å¸§ç‡æ§åˆ¶
        if (current_time - last_frame_time) < frame_time:
            time.sleep(frame_time - (current_time - last_frame_time))
            continue
        
        ret, frame = cap.read()
        if not ret:
            print("[WARN] Failed to read frame, retrying...")
            time.sleep(0.1)
            continue
            
        last_frame_time = current_time
        frame_count += 1
        
        # åªåœ¨ç‰¹å®šå¸§æ‰§è¡ŒAIå¤„ç†
        detected_objects = []
        face_detections = []  # æ”¶é›†å½“å‰å¸§çš„äººè„¸æ£€æµ‹ç»“æœ
        
        if frame_count % ai_process_interval == 0:
            # æ ¹æ®è‡ªåŠ¨æ¨¡å¼é˜¶æ®µå†³å®šä½¿ç”¨å“ªä¸ªAIæ¨¡å‹
            if AUTO_MODE_ENABLED:
                if auto_stage in ["SEARCHING_PERSON", "SERVING"]:
                    # äººè„¸è¯†åˆ«é˜¶æ®µï¼šåªä½¿ç”¨face_recognition
                    # print(f"[AI] Using FACE RECOGNITION model in stage: {auto_stage}")  # å‡å°‘æ—¥å¿—è¾“å‡º
                    if known_encodings:
                        try:
                            # ç¡®ä¿åœ¨äººè„¸è¯†åˆ«é˜¶æ®µæ‘„åƒå¤´æ˜¯æŠ¬èµ·çš„
                            control_camera_angle("up")
                            
                            # ä½¿ç”¨å°å°ºå¯¸å›¾åƒè¿›è¡Œäººè„¸è¯†åˆ«ï¼Œæé«˜é€Ÿåº¦
                            small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                            rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                            face_locations = face_recognition.face_locations(rgb_frame, model='hog')
                            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

                            for (top, right, bottom, left), encoding in zip(face_locations, face_encodings):
                                # å°†åæ ‡æ”¾å¤§å›åŸå§‹å°ºå¯¸
                                top *= 2
                                right *= 2
                                bottom *= 2
                                left *= 2
                                
                                matches = face_recognition.compare_faces(known_encodings, encoding, tolerance=FACE_CONFIDENCE_THRESHOLD)
                                name = "Unknown"
                                preference = "No preference"
                                
                                if True in matches:
                                    best_index = np.argmin(face_recognition.face_distance(known_encodings, encoding))
                                    if matches[best_index]:
                                        name = known_names[best_index]
                                        preference = database[name]["preference"]
                                        face_detected = True
                                        
                                        # è‡ªåŠ¨æ¨¡å¼ä¸‹çš„äººè„¸è¯†åˆ«å“åº”
                                        if name != "Unknown":
                                            if (current_time - last_recognition_time > RECOGNITION_COOLDOWN and 
                                                recognized_person != name):
                                                recognized_person = name
                                                last_recognition_time = current_time
                                                print(f"[AUTO] Recognized {name}, preference: {preference}")
                                                
                                                # åˆ‡æ¢åˆ°360åº¦æ—‹è½¬æœç´¢é¥®æ–™é˜¶æ®µ
                                                auto_stage = "ROTATING_SEARCH"
                                                rotation_angle = 0
                                                last_rotation_time = current_time  # åˆå§‹åŒ–æ—‹è½¬æ—¶é—´
                                                control_camera_angle("down")  # æ‘„åƒå¤´æ”¾ä¸‹å¯»æ‰¾é¥®æ–™
                                                print(f"[AUTO] Starting 360Â° rotation search for {preference}")
                                                print(f"[AUTO DEBUG] Initial rotation_angle: {rotation_angle}, last_rotation_time: {last_rotation_time}")
                                                
                                                # ç«‹å³å¼€å§‹ç¬¬ä¸€æ¬¡æ—‹è½¬
                                                print("[AUTO] Starting first rotation...")
                                                rotate_robot("right", 0.8)
                                                rotation_angle += ROTATION_STEP
                                                last_rotation_time = current_time

                                # æ”¶é›†æ£€æµ‹ç»“æœ
                                face_detections.append((name, (left, top, right, bottom), preference))
                        except Exception as e:
                            print(f"[WARN] Face recognition error: {e}")
                            
                elif auto_stage in ["ROTATING_SEARCH", "MOVING_TO_DRINK"]:
                    # é¥®æ–™æœç´¢é˜¶æ®µï¼šåªä½¿ç”¨YOLO
                    # print(f"[AI] Using YOLO model in stage: {auto_stage}")  # å‡å°‘æ—¥å¿—è¾“å‡º
                    if yolo_model:
                        try:
                            results = yolo_model(frame, verbose=False)
                            for result in results:
                                if result.boxes is not None:
                                    for box in result.boxes:
                                        cls = int(box.cls[0])
                                        label = yolo_model.names[cls]
                                        conf = box.conf[0]
                                        xyxy = box.xyxy[0].cpu().numpy().astype(int)

                                        # åªæ˜¾ç¤ºç½®ä¿¡åº¦è¾ƒé«˜çš„æ£€æµ‹ç»“æœ
                                        if conf > 0.5:
                                            detected_objects.append(label)
                                            
                                            # ç”»æ¡†
                                            x1, y1, x2, y2 = xyxy
                                            
                                            # æ ¹æ®ç‰©ä½“ç±»å‹ä½¿ç”¨ä¸åŒé¢œè‰²
                                            if label in ['bottle', 'cup', 'wine glass', 'bowl']:
                                                color = (255, 0, 0)  # é¥®æ–™ç›¸å…³ç‰©ä½“ç”¨è“è‰²
                                            elif label in ['person']:
                                                color = (0, 255, 0)  # äººç”¨ç»¿è‰²
                                            else:
                                                color = (0, 255, 255)  # å…¶ä»–ç‰©ä½“ç”¨é»„è‰²
                                            
                                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                                            
                                            # æ·»åŠ æ ‡ç­¾èƒŒæ™¯
                                            label_text = f"{label} {conf:.2f}"
                                            (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                                            cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
                                            cv2.putText(frame, label_text, (x1, y1 - 5),
                                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                        except Exception as e:
                            print(f"[WARN] YOLO detection error: {e}")
                            
            else:
                # æ‰‹åŠ¨æ¨¡å¼ï¼šåŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
                # print(f"[AI] Manual mode - using both models")  # æ³¨é‡Šæ‰é¿å…åˆ·å±
                
                # ============ YOLO ç‰©ä½“æ£€æµ‹ ============
                if yolo_model:
                    try:
                        results = yolo_model(frame, verbose=False)
                        for result in results:
                            if result.boxes is not None:
                                for box in result.boxes:
                                    cls = int(box.cls[0])
                                    label = yolo_model.names[cls]
                                    conf = box.conf[0]
                                    xyxy = box.xyxy[0].cpu().numpy().astype(int)

                                    # åªæ˜¾ç¤ºç½®ä¿¡åº¦è¾ƒé«˜çš„æ£€æµ‹ç»“æœ
                                    if conf > 0.5:
                                        detected_objects.append(label)
                                        
                                        # ç”»æ¡†
                                        x1, y1, x2, y2 = xyxy
                                        
                                        # æ ¹æ®ç‰©ä½“ç±»å‹ä½¿ç”¨ä¸åŒé¢œè‰²
                                        if label in ['bottle', 'cup', 'wine glass', 'bowl']:
                                            color = (255, 0, 0)  # é¥®æ–™ç›¸å…³ç‰©ä½“ç”¨è“è‰²
                                        elif label in ['person']:
                                            color = (0, 255, 0)  # äººç”¨ç»¿è‰²
                                        else:
                                            color = (0, 255, 255)  # å…¶ä»–ç‰©ä½“ç”¨é»„è‰²
                                        
                                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                                        
                                        # æ·»åŠ æ ‡ç­¾èƒŒæ™¯
                                        label_text = f"{label} {conf:.2f}"
                                        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                                        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
                                        cv2.putText(frame, label_text, (x1, y1 - 5),
                                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                    except Exception as e:
                        print(f"[WARN] YOLO detection error: {e}")

                # ============ äººè„¸è¯†åˆ«ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰============
                face_detected = False
                
                if known_encodings:
                    try:
                        # ä½¿ç”¨å°å°ºå¯¸å›¾åƒè¿›è¡Œäººè„¸è¯†åˆ«ï¼Œæé«˜é€Ÿåº¦
                        small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                        rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                        face_locations = face_recognition.face_locations(rgb_frame, model='hog')
                        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

                        for (top, right, bottom, left), encoding in zip(face_locations, face_encodings):
                            # å°†åæ ‡æ”¾å¤§å›åŸå§‹å°ºå¯¸
                            top *= 2
                            right *= 2
                            bottom *= 2
                            left *= 2
                            
                            matches = face_recognition.compare_faces(known_encodings, encoding, tolerance=FACE_CONFIDENCE_THRESHOLD)
                            name = "Unknown"
                            preference = "No preference"
                            
                            if True in matches:
                                best_index = np.argmin(face_recognition.face_distance(known_encodings, encoding))
                                if matches[best_index]:
                                    name = known_names[best_index]
                                    preference = database[name]["preference"]
                                    face_detected = True

                            # æ”¶é›†æ£€æµ‹ç»“æœ
                            face_detections.append((name, (left, top, right, bottom), preference))
                    except Exception as e:
                        print(f"[WARN] Face recognition error: {e}")
            
            # ============ è‡ªåŠ¨æ¨¡å¼çŠ¶æ€æœºå¤„ç† ============
            if AUTO_MODE_ENABLED:
                try:
                    if auto_stage == "ROTATING_SEARCH":
                        # 360åº¦æ—‹è½¬æœç´¢é¥®æ–™
                        print(f"[AUTO DEBUG] ROTATING_SEARCH - Current time: {current_time:.2f}, Last rotation: {last_rotation_time:.2f}, Diff: {current_time - last_rotation_time:.2f}, Delay: {ROTATION_DELAY}")
                        
                        if current_time - last_rotation_time > ROTATION_DELAY:
                            # æ£€æŸ¥å½“å‰è§’åº¦æ˜¯å¦å‘ç°äº†ç›®æ ‡é¥®æ–™
                            if recognized_person and database.get(recognized_person):
                                user_preference = database[recognized_person]["preference"]
                                print(f"[AUTO DEBUG] Checking for drinks. Detected objects: {detected_objects}")
                                print(f"[AUTO DEBUG] User preference: {user_preference}")
                                
                                if check_for_target_drink(detected_objects, user_preference):
                                    print(f"[AUTO] Found target drink at angle {rotation_angle}Â°!")
                                    drink_found_position = rotation_angle
                                    auto_stage = "MOVING_TO_DRINK"
                                    print("[AUTO] Moving towards the drink...")
                                    # å¼€å§‹å‰è¿›åˆ°é¥®æ–™ä½ç½®
                                    move_robot("forward", 0.5)
                                else:
                                    # å…ˆæ—‹è½¬åˆ°ä¸‹ä¸€ä¸ªè§’åº¦ï¼Œå†æ›´æ–°è§’åº¦å€¼
                                    print(f"[AUTO] No target drink found at {rotation_angle}Â°. Detected: {detected_objects}")
                                    print(f"[AUTO] Rotating to next position...")
                                    rotate_robot("right", 0.8)  # 45åº¦å¤§çº¦éœ€è¦0.8ç§’
                                    
                                    # æ›´æ–°è§’åº¦
                                    rotation_angle += ROTATION_STEP
                                    if rotation_angle >= 360:
                                        # å®Œæˆä¸€åœˆæœç´¢ä½†æ²¡æ‰¾åˆ°ï¼Œé‡ç½®æœç´¢
                                        print("[AUTO] Completed 360Â° search, no target drink found. Restarting...")
                                        rotation_angle = 0
                                        auto_stage = "SEARCHING_PERSON"
                                        last_rotation_time = current_time  # é‡ç½®æ—‹è½¬æ—¶é—´
                                        control_camera_angle("up")
                                    else:
                                        print(f"[AUTO] Now at {rotation_angle}Â° (Step {rotation_angle//ROTATION_STEP}/8)")
                                    
                                    last_rotation_time = current_time
                            else:
                                # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°äººå‘˜ï¼Œé‡ç½®çŠ¶æ€
                                print("[AUTO] No recognized person in ROTATING_SEARCH, resetting...")
                                auto_stage = "SEARCHING_PERSON"
                                control_camera_angle("up")
                                last_rotation_time = current_time
                        else:
                            print(f"[AUTO DEBUG] Waiting... Time remaining: {ROTATION_DELAY - (current_time - last_rotation_time):.2f}s")
                    
                    elif auto_stage == "MOVING_TO_DRINK":
                        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘é¥®æ–™ï¼ˆè¿™é‡Œä¾èµ–Arduinoçš„è¶…å£°æ³¢è‡ªåŠ¨æŠ“å–ï¼‰
                        # Arduinoä¼šåœ¨æ£€æµ‹åˆ°6cmè·ç¦»æ—¶è‡ªåŠ¨æŠ“å–
                        # æˆ‘ä»¬åªéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´è®©Arduinoå®ŒæˆæŠ“å–
                        if not hasattr(control_camera_angle, 'grab_start_time'):
                            control_camera_angle.grab_start_time = current_time
                        
                        if current_time - control_camera_angle.grab_start_time > 8:  # ç­‰å¾…8ç§’è®©Arduinoå®ŒæˆæŠ“å–
                            print("[AUTO] Drink should be grabbed, returning to customer...")
                            auto_stage = "RETURNING"
                            delattr(control_camera_angle, 'grab_start_time')
                            # å¼€å§‹è¿”å›åŸç‚¹
                            return_to_origin()
                    
                    elif auto_stage == "RETURNING":
                        # è¿”å›åˆ°åŸç‚¹é¢å‘é¡¾å®¢
                        if not hasattr(control_camera_angle, 'return_start_time'):
                            control_camera_angle.return_start_time = current_time
                            
                        if current_time - control_camera_angle.return_start_time > 5:  # ç­‰å¾…5ç§’å®Œæˆè¿”å›
                            print("[AUTO] Returned to customer, preparing to serve drink...")
                            auto_stage = "SERVING"
                            delattr(control_camera_angle, 'return_start_time')
                            control_camera_angle("up")  # æ‘„åƒå¤´æŠ¬èµ·é¢å‘é¡¾å®¢
                            
                    elif auto_stage == "SERVING":
                        # é€’é€é¥®æ–™ç»™é¡¾å®¢
                        if not hasattr(control_camera_angle, 'serve_start_time'):
                            control_camera_angle.serve_start_time = current_time
                            serve_drink()  # æ‰§è¡Œé€’é€åŠ¨ä½œ
                            
                        if current_time - control_camera_angle.serve_start_time > 5:  # ç­‰å¾…5ç§’å®Œæˆé€’é€
                            print(f"[AUTO] Service completed for {recognized_person}!")
                            # é‡ç½®çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡æœåŠ¡
                            auto_stage = "SEARCHING_PERSON"
                            recognized_person = None
                            drink_found_position = None
                            rotation_angle = 0
                            last_rotation_time = current_time  # é‡ç½®æ—‹è½¬æ—¶é—´
                            delattr(control_camera_angle, 'serve_start_time')
                            face_tracking_data.clear()
                            
                except Exception as e:
                    print(f"[AUTO ERROR] State machine error: {e}")
                    # å‘ç”Ÿé”™è¯¯æ—¶é‡ç½®åˆ°æœç´¢äººå‘˜çŠ¶æ€
                    auto_stage = "SEARCHING_PERSON"
                    last_rotation_time = time.time()  # é‡ç½®æ—‹è½¬æ—¶é—´
                    control_camera_angle("up")
            
            # æ›´æ–°äººè„¸è·Ÿè¸ªæ•°æ®å¹¶ç»˜åˆ¶ç¨³å®šçš„äººè„¸æ¡†
            try:
                # æ— è®ºæ˜¯å¦æœ‰æ£€æµ‹ç»“æœéƒ½è¦æ›´æ–°è·Ÿè¸ªæ•°æ®
                update_face_tracking(face_detections, frame_count)
            except Exception as e:
                print(f"[WARN] Face tracking error: {e}")
            
            # å¦‚æœåœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹æ²¡æœ‰æ£€æµ‹åˆ°å·²çŸ¥äººè„¸ï¼Œé‡ç½®è¯†åˆ«çŠ¶æ€
            if AUTO_MODE_ENABLED and not face_detected and recognized_person:
                if current_time - last_recognition_time > RECOGNITION_COOLDOWN * 2:
                    recognized_person = None
                    auto_stage = "SEARCHING_PERSON"  # é‡æ–°å¼€å§‹å¯»æ‰¾äººè„¸
                    last_rotation_time = current_time  # é‡ç½®æ—‹è½¬æ—¶é—´
                    control_camera_angle("up")  # æ‘„åƒå¤´é‡æ–°æŠ¬èµ·
                    # æ¸…ç©ºäººè„¸è·Ÿè¸ªæ•°æ®
                    face_tracking_data.clear()
                    print("[AUTO] No known face detected, resetting to person search mode")
        
        # ç»˜åˆ¶ç¨³å®šçš„äººè„¸æ¡†ï¼ˆæ¯å¸§éƒ½æ˜¾ç¤ºï¼Œå‡å°‘é—ªçƒï¼‰
        try:
            stable_faces = get_stable_faces()
            for name, (left, top, right, bottom), preference in stable_faces:
                # ç”»äººè„¸æ¡†
                color = (0, 255, 255) if name != "Unknown" else (0, 0, 255)
                cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
                
                # æ˜¾ç¤ºå§“åå’Œåå¥½
                label = f"{name}"
                if name != "Unknown":
                    label += f" ({preference})"
                
                # æ–‡æœ¬èƒŒæ™¯
                (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(frame, (left, top - text_height - 10), (left + text_width, top), color, -1)
                
                # æ–‡æœ¬
                cv2.putText(frame, label, (left, top - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        except Exception as e:
            print(f"[WARN] Face drawing error: {e}")

        # æ˜¾ç¤ºæ¨¡å¼å’ŒçŠ¶æ€ä¿¡æ¯ï¼ˆæ¯å¸§éƒ½æ˜¾ç¤ºï¼‰
        mode_text = "AUTO" if AUTO_MODE_ENABLED else "MANUAL"
        cv2.putText(frame, f"Mode: {mode_text}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # æ˜¾ç¤ºæ‘„åƒå¤´ä½ç½®
        camera_text = f"Camera: {camera_position.upper()}"
        cv2.putText(frame, camera_text, (10, frame.shape[0] - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ˜¾ç¤ºè‡ªåŠ¨æ¨¡å¼é˜¶æ®µ
        if AUTO_MODE_ENABLED:
            try:
                stage_display = auto_stage.replace('_', ' ').title()
                if auto_stage == "ROTATING_SEARCH":
                    stage_display += f" ({rotation_angle}Â°)"
                elif auto_stage == "MOVING_TO_DRINK" and drink_found_position:
                    stage_display += f" (Found at {drink_found_position}Â°)"
                
                stage_text = f"Stage: {stage_display}"
                cv2.putText(frame, stage_text, (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            except Exception as e:
                print(f"[WARN] Auto stage display error: {e}")
                cv2.putText(frame, "Stage: Unknown", (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        if recognized_person:
            cv2.putText(frame, f"Serving: {recognized_person}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„ç‰©ä½“æ•°é‡
        if detected_objects:
            unique_objects = list(set(detected_objects))
            objects_text = f"Objects: {', '.join(unique_objects[:3])}"
            if len(unique_objects) > 3:
                objects_text += f" +{len(unique_objects)-3} more"
            cv2.putText(frame, objects_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # æ˜¾ç¤ºæ¨¡å‹ç±»å‹
        model_text = f"Model: {model_type.upper()}"
        cv2.putText(frame, model_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # ============ çº¿ç¨‹å®‰å…¨çš„GUIæ›´æ–° ============
        try:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame)
            imgtk = ImageTk.PhotoImage(image=img)
            
            # ä½¿ç”¨afteræ–¹æ³•åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°GUI
            def update_gui():
                if not stop_event.is_set():
                    video_label.configure(image=imgtk)
                    video_label.imgtk = imgtk
            
            root.after(0, update_gui)
        except Exception as e:
            if not stop_event.is_set():
                print(f"[WARN] GUI update error: {e}")

    cap.release()
    print("[INFO] Video capture released")


# ---------------------------
# GUI åˆ›å»º
# ---------------------------
root = tk.Tk()
root.title("Smart Robot Control Panel")
root.geometry("1000x800")  # å¢åŠ çª—å£å¤§å°
root.bind("<KeyPress>", on_key_press)
root.bind("<KeyRelease>", on_key_release)
root.focus_set()  # ç¡®ä¿çª—å£è·å¾—ç„¦ç‚¹ä»¥æ¥æ”¶é”®ç›˜äº‹ä»¶

# æ ‡é¢˜
title_label = ttk.Label(root, text="Smart Robot Control Panel", font=('Arial', 16, 'bold'))
title_label.pack(pady=10)

# æ¨¡å¼é€‰æ‹©æŒ‰é’®
mode_var = tk.StringVar(value='manual')
ttk.Label(root, text="Control Mode:", font=('Arial', 14)).pack(pady=10)

mode_frame = ttk.Frame(root)
mode_frame.pack()

def set_manual():
    mode_var.set('manual')
    switch_mode('manual')
    status_label.config(text="Status: Manual Mode - Use WASD keys to control")

def set_auto():
    global auto_stage, rotation_angle, drink_found_position, last_rotation_time
    mode_var.set('auto')
    switch_mode('auto')
    auto_stage = "SEARCHING_PERSON"  # é‡ç½®åˆ°å¯»æ‰¾äººè„¸é˜¶æ®µ
    rotation_angle = 0  # é‡ç½®æ—‹è½¬è§’åº¦
    drink_found_position = None  # é‡ç½®é¥®æ–™å‘ç°ä½ç½®
    last_rotation_time = time.time()  # åˆå§‹åŒ–æ—‹è½¬æ—¶é—´ä¸ºå½“å‰æ—¶é—´
    control_camera_angle("up")  # æ‘„åƒå¤´æŠ¬èµ·å‡†å¤‡äººè„¸è¯†åˆ«
    print(f"[AUTO DEBUG] Auto mode initialized - auto_stage: {auto_stage}, rotation_angle: {rotation_angle}, last_rotation_time: {last_rotation_time}")
    status_label.config(text="Status: Auto Mode - Searching for person (Camera UP)")

ttk.Button(mode_frame, text="Manual Mode", command=set_manual, width=20).grid(row=0, column=0, padx=10)
ttk.Button(mode_frame, text="Auto Mode", command=set_auto, width=20).grid(row=0, column=1, padx=10)

# æ‰‹åŠ¨æ§åˆ¶æŒ‰é’®
control_frame = ttk.LabelFrame(root, text="Manual Controls", padding=10)
control_frame.pack(pady=10)

def send_manual_command(cmd):
    if not AUTO_MODE_ENABLED:
        send_robot_command(cmd)

ttk.Button(control_frame, text="â†‘ Forward", command=lambda: send_manual_command("MOVE:FORWARD")).grid(row=0, column=1, padx=5, pady=5)
ttk.Button(control_frame, text="â† Left", command=lambda: send_manual_command("TURN:LEFT")).grid(row=1, column=0, padx=5, pady=5)
ttk.Button(control_frame, text="Stop", command=lambda: send_manual_command("STATUS:Idle")).grid(row=1, column=1, padx=5, pady=5)
ttk.Button(control_frame, text="â†’ Right", command=lambda: send_manual_command("TURN:RIGHT")).grid(row=1, column=2, padx=5, pady=5)
ttk.Button(control_frame, text="â†“ Backward", command=lambda: send_manual_command("MOVE:BACKWARD")).grid(row=2, column=1, padx=5, pady=5)

# æ‘„åƒå¤´æ§åˆ¶æŒ‰é’®
def manual_camera_up():
    if not AUTO_MODE_ENABLED:
        control_camera_angle("up")
        status_label.config(text="Status: Camera moved UP")

def manual_camera_down():
    if not AUTO_MODE_ENABLED:
        control_camera_angle("down")
        status_label.config(text="Status: Camera moved DOWN")

ttk.Button(control_frame, text="ğŸ“¹ Camera UP", command=manual_camera_up).grid(row=0, column=0, padx=5, pady=5)
ttk.Button(control_frame, text="ğŸ“¹ Camera DOWN", command=manual_camera_down).grid(row=2, column=0, padx=5, pady=5)

# æµ‹è¯•æ—‹è½¬æŒ‰é’®
def test_rotate_left():
    if not AUTO_MODE_ENABLED:
        rotate_robot("left", 0.8)
        status_label.config(text="Status: Test rotation LEFT")

def test_rotate_right():
    if not AUTO_MODE_ENABLED:
        rotate_robot("right", 0.8)
        status_label.config(text="Status: Test rotation RIGHT")

ttk.Button(control_frame, text="ğŸ”„ Test Left", command=test_rotate_left).grid(row=0, column=3, padx=5, pady=5)
ttk.Button(control_frame, text="ğŸ”„ Test Right", command=test_rotate_right).grid(row=1, column=3, padx=5, pady=5)

# æµ‹è¯•360åº¦æœç´¢æŒ‰é’®
def test_full_rotation():
    """æµ‹è¯•å®Œæ•´çš„360åº¦æ—‹è½¬æœç´¢"""
    global AUTO_MODE_ENABLED, auto_stage, rotation_angle, last_rotation_time, recognized_person, database
    
    if not AUTO_MODE_ENABLED:
        print("[TEST] Starting 360Â° rotation test...")
        
        # åˆ›å»ºæµ‹è¯•ç”¨æˆ·æ•°æ®
        if "Test User" not in database:
            database["Test User"] = {"preference": "bottle"}  # æµ‹è¯•å¯»æ‰¾ç“¶å­
        
        auto_stage = "ROTATING_SEARCH"
        rotation_angle = 0
        last_rotation_time = time.time()
        recognized_person = "Test User"  # æ¨¡æ‹Ÿè¯†åˆ«åˆ°ç”¨æˆ·
        control_camera_angle("down")  # æ‘„åƒå¤´æ”¾ä¸‹
        
        # æš‚æ—¶åˆ‡æ¢åˆ°è‡ªåŠ¨æ¨¡å¼è¿›è¡Œæµ‹è¯•
        AUTO_MODE_ENABLED = True
        status_label.config(text="Status: Testing 360Â° rotation for bottles...")
        
        # ç«‹å³å¼€å§‹ç¬¬ä¸€æ¬¡æ—‹è½¬
        print("[TEST] Starting first test rotation...")
        rotate_robot("right", 5)
        rotation_angle += ROTATION_STEP
        last_rotation_time = time.time()
        
        # 25ç§’åæ¢å¤æ‰‹åŠ¨æ¨¡å¼
        def restore_manual():
            global AUTO_MODE_ENABLED, auto_stage
            AUTO_MODE_ENABLED = False
            auto_stage = "SEARCHING_PERSON"
            control_camera_angle("up")  # æ‘„åƒå¤´æŠ¬èµ·
            status_label.config(text="Status: Manual Mode - Test completed")
        root.after(25000, restore_manual)  # 25ç§’åæ¢å¤æ‰‹åŠ¨æ¨¡å¼

ttk.Button(control_frame, text="ğŸ”„ Test 360Â°", command=test_full_rotation).grid(row=2, column=3, padx=5, pady=5)

# æ¨¡å‹æ§åˆ¶æŒ‰é’®
def reload_yolo_model():
    global yolo_model, model_type
    try:
        custom_model_path = "yolo_weights/best.pt"
        if os.path.exists(custom_model_path):
            yolo_model = YOLO(custom_model_path)
            model_type = "custom"
            model_info = "YOLO Model: Custom (Beverage Detection)"
            print("[INFO] Switched to custom beverage detection model")
        else:
            yolo_model = YOLO('yolov8n.pt')
            model_type = "official"
            model_info = "YOLO Model: Official (General Object Detection)"
            print("[INFO] Using official YOLOv8n model")
        
        model_label.config(text=model_info)
        status_label.config(text=f"Status: Model reloaded - {model_type}")
    except Exception as e:
        print(f"[ERROR] Failed to reload model: {e}")
        status_label.config(text="Status: Model reload failed")

ttk.Button(control_frame, text="Reload Model", command=reload_yolo_model).grid(row=3, column=1, padx=5, pady=5)

# çŠ¶æ€æ˜¾ç¤º
status_label = ttk.Label(root, text="Status: Manual Mode - Use WASD keys to control", font=('Arial', 10))
status_label.pack(pady=5)

# è¯´æ˜æ–‡å­—
instructions = ttk.Label(root, text="Keyboard: W=Forward, S=Backward, A=Left, D=Right, G/R=Gripper, U/J=Up/Down", 
                        font=('Arial', 9), foreground='gray')
instructions.pack(pady=5)

# æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º
if yolo_model:
    model_info = f"YOLO Model: {model_type.title()} ({'Custom Beverage Detection' if model_type == 'custom' else 'General Object Detection'})"
else:
    model_info = "YOLO Model: Not loaded"
    
model_label = ttk.Label(root, text=model_info, font=('Arial', 9), foreground='blue')
model_label.pack(pady=5)

# è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
video_label = tk.Label(root)
video_label.pack(pady=20)

# åœæ­¢çº¿ç¨‹çš„äº‹ä»¶æ ‡å¿—
stop_event = threading.Event()
video_thread = threading.Thread(target=update_video, daemon=True)
video_thread.start()

# é€€å‡ºæ¸…ç†
def on_close():
    stop_event.set()
    root.destroy()

root.protocol("WM_DELETE_WINDOW", on_close)
root.mainloop()
